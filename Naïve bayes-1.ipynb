{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d172d0c-af33-41ba-be0d-f39c296d7f1f",
   "metadata": {},
   "source": [
    "Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3aa38-3cb1-427d-92b8-6eeded8c2f56",
   "metadata": {},
   "source": [
    "### **Bayes' Theorem**:\r\n",
    "Bayes' theorem is a fundamental concept in probability theory that describes how to update the probability of a hypothesis, \\( H \\), based on new evidence or data, \\( E \\). It provides a mathematical framework for reasoning about uncertainty and revising beliefs in light of new information.\r\n",
    "\r\n",
    "The formula for **Bayes' Theorem** isP(H/E)= P(E/H).P(H)/P(E)E)}\r\n",
    "\\]\r\n",
    "\r\n",
    "Where:\r\n",
    "- **\\( P(H|E) \\)**: The **posterior probability**. It is the probability of the hypothesis \\( H \\) being true given the new evidence \\( E \\). This is what we're trying to compute.\r\n",
    "- **\\( P(E|H) \\)**: The **likelihood**. It is the probability of observing the evidence \\( E \\) given that the hypothesis \\( H \\) is true.\r\n",
    "- **\\( P(H) \\)**: The **prior probability** of the hypothesis \\( H \\). This is the initial probability of \\( H \\) before considering the new evidence.\r\n",
    "- **\\( P(E) \\)**: The **marginal likelihood** or **evidence**. It is the total probability of the evidence \\( E \\) under all possible hypotheses.\r\n",
    "\r\n",
    "### **Explanation**:\r\n",
    "- **Prior Probability** \\( P(H) \\): Represents our initial belief about the hypothesis before seeing any evidence.\r\n",
    "- **Likelihood** \\( P(E|H) \\): Represents how likely the observed evidence is, assuming the hypothesis is true.\r\n",
    "- **Posterior Probability** \\( P(H|E) \\): Represents our updated belief about the hypothesis after taking into account the new evidence.\r\n",
    "- **Marginal Probability** \\( P(E) \\): Normalizes the posterior, ensuring that it is a valid probability distribution. It is the overall probability of the evidence across all hypotheses.\r\n",
    "\r\n",
    "### **Example**:\r\n",
    "Suppose a doctor is trying to determine whether a patient has a particular disease (\\( H \\)) based on the result of a medical test (\\( E \\)). The doctor knows:\r\n",
    "- The **prior probability** \\( P(H) \\), which is the probability of the patient having the disease (based on general population data).\r\n",
    "- The **likelihood** \\( P(E|H) \\), which is the probability of a positive test result if the patient actually has the disease.\r\n",
    "- The **marginal probability** \\( P(E) \\), which is the overall probability of a positive test result (including false positives).\r\n",
    "\r\n",
    "Using Bayes' theorem, the doctor can calculate the **posterior probability** \\( P(H|E) \\), which is the updated probability that the patient has the disease after seeing the test result.\r\n",
    "\r\n",
    "Bayes' theorem is widely used in fields like machine learning, statistics, medicine, and decision-making under uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc76f4-c1a0-479f-849d-436f22535abc",
   "metadata": {},
   "source": [
    "Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e43d6-7399-4d95-ab40-50bd14db571b",
   "metadata": {},
   "source": [
    "P(A|B) = (P(B|A) * P(A)) / P(B)\r\n",
    "\r\n",
    "Where:\r\n",
    "\r\n",
    "P(A|B) represents the conditional probability of event A occurring given that event B has occurred. This is the probability we want to calculate.\r\n",
    "P(B|A) is the conditional probability of event B occurring given that event A has occurred.\r\n",
    "P(A) is the prior probability of event A occurring independently of any information about event B.\r\n",
    "P(B) is the prior probability of event B occurring independently of any information about event A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4970da2d-6b81-4e4e-8d61-b822d2c1ab16",
   "metadata": {},
   "source": [
    "Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ff425-b254-4ba5-ac9f-3cfbc406b7a9",
   "metadata": {},
   "source": [
    "Say we have a set of independent features age,height and weight and overweight as the output feature,\r\n",
    "\r\n",
    "Here age,height and weight are independent feature and overweight is the dependent feature\r\n",
    "\r\n",
    "age   height   weight   overweight\r\n",
    "17     170       73         no\r\n",
    "21     165       91         yes\r\n",
    "28     178       88         yes\r\n",
    "42     160       60         no\r\n",
    "\r\n",
    "Now we have to find if the pearson is overweight or not based on age height and weight,\r\n",
    "\r\n",
    "Considering,\r\n",
    "\r\n",
    "Age,height and weight as x1,x2 amd x3 respectively and\r\n",
    "overweight as y\r\n",
    "By Using\n",
    "\n",
    "P(y/(x1,x2,x3) = (P(y) * P(x1,x2,x3)/y ) / P(x1,x2,x3)\r\n",
    "P(y/(x1,x2,x3) = [(P(y) * P(x1/y) * P(x2/y) * P(x3)/y ] / P(x1,x2,x3)\r\n",
    "P(yes/(x1,x2,x3) = [(P(yes) * P(x1/yes) * P(x2/yes) * P(x3/yes) ] / P(x1,x2,x3)\r\n",
    "P(no/(x1,x2,x3) = [(P(no) * P(x1/no) * P(x2/no) * P(x3/no) ] / P(x1,x2,x3)\r\n",
    "\r\n",
    "The Denominator P(x1,x2,x3) is contant and can be ignored in calculation\r\n",
    "\r\n",
    "Final Formula is ,\r\n",
    "\r\n",
    "P(yes/(x1,x2,x3) = (P(yes) * P(x1/yes) * P(x2/yes) * P(x3/yes)\r\n",
    "P(no/(x1,x2,x3) = (P(no) * P(x1/no) * P(x2/no) * P(x3/no)\r\n",
    "\r\n",
    "The one whose probabilty is the highest is considered the output. Bayes Theorem ,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03443ff5-af58-4452-87be-1c1f6ab57586",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c49de-e603-42d3-b3d4-5ee5176a716b",
   "metadata": {},
   "source": [
    "Bayes' theorem is a mathematical formula that describes how to calculate conditional probabilities, and it provides a framework for updating probability beliefs based on new information or evidence.\r\n",
    "\r\n",
    "The relationship between Bayes' theorem and conditional probability is as follows:1.\r\n",
    "\r\n",
    "Conditional Probability: Conditional probability is a fundamental concept that deals with the probability of an event occurring given that another event has already occurred. It's denoted as P(A|B), which represents the probability of event A occurring given that event B has occurre2.d.\r\n",
    "\r\n",
    "Bayes' Theorem: Bayes' theorem is a specific formula that allows you to calculate conditional probabilities. It relates the conditional probability P(A|B) to other probabilities, including the prior probability of A (P(A)), the prior probability of B (P(B)), and the likelihood of B given A (P(B|A)). The formula is as follows:\r\n",
    "\r\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\r\n",
    "\r\n",
    "P(A|B) is the conditional probability you want to calculate, and it's related to the other probabilities in the equation.\r\n",
    "Bayes' theorem is a mathematical tool for calculating conditional probabilities, and it formalizes the process of updating probability beliefs in light of new evidence. It's a fundamental concept in Bayesian probability and statistics, and it plays a crucial role in various applications, such as machine learning, and decision-making under uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4221f3b-f221-4a60-b602-081685152285",
   "metadata": {},
   "source": [
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b01bd6-f39b-4ae4-9d51-107667b2df1d",
   "metadata": {},
   "source": [
    "There are 3 Types of Naive Bayes classifiers :\r\n",
    "1.\r\n",
    "Gaussian Naive Bayes (GNB):\r\n",
    "\r\n",
    "Data Type: GNB is suitable for continuous or real-valued data.\r\n",
    "Assumptions: It assumes that the features follow a Gaussian (normal) distribution.\r\n",
    "Example Applications: GNB is often used for problems involving continuous features, such as spam detection (with word frequencies as features) or medical diagnosis (with physiological measurem\n",
    "e2.nts).\r\n",
    "Multinomial Naive Bayes (MNB):\r\n",
    "\r\n",
    "Data Type: MNB is commonly used with discrete data, especially when dealing with text data.\r\n",
    "Assumptions: It assumes that the features represent the counts or frequencies of events (e.g., word counts in text documents).\r\n",
    "Example Applications: MNB is well-suited for text classification problems like sentiment analysis or document categorization, where features often represent word counts or term f\n",
    "r3.equencies.\r\n",
    "Bernoulli Naive Bayes (BNB):\r\n",
    "\r\n",
    "Data Type: BNB is also used with discrete data, but it's tailored for binary features (i.e., features that are either present or absent).\r\n",
    "Assumptions: It assumes that features are binary, where 1 represents the presence of a feature and 0 represents the absence.\r\n",
    "Example Applications: BNB is commonly applied in text classification tasks where binary features indicate whether specific words appear in a document or not (e.g., spam or not spam classification based on the presence of certain words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204398dc-7182-4dfe-82fa-88fa48d8c638",
   "metadata": {},
   "source": [
    "Q6. Assignment:\r\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\r\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\r\n",
    "each feature value for each class:\r\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2    =4    \r\n",
    "A    3    3     4   4    3 3         3\r\n",
    "   B    2     2   1    2 2 2 3\r\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\r\n",
    "to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c60f24-360e-4d99-93d4-ec1ed24373d8",
   "metadata": {},
   "source": [
    "Let's denote:\r\n",
    "\r\n",
    "P(A) as the prior probability of class A (given as equal to P(B) in this case because of equal prior probabilities for each class).\r\n",
    "P(X1 = 3 | A) as the conditional probability of observing X1 = 3 given class A.\r\n",
    "P(X2 = 4 | A) as the conditional probability of observing X2 = 4 given class A.\r\n",
    "P(X1 = 3 | B) as the conditional probability of observing X1 = 3 given class B.\r\n",
    "P(X2 = 4 | B) as the conditional probability of observing X2 = 4 given class B.\r\n",
    "You can calculate these conditional probabilities based on the provided frequency table:\r\n",
    "\r\n",
    "P(X1 = 3 | A) = 4 / (4 + 3) = 4/7\r\n",
    "P(X2 = 4 | A) = 3 / (4 + 3) = 3/7\r\n",
    "P(X1 = 3 | B) = 1 / (1 + 2) = 1/3\r\n",
    "P(X2 = 4 | B) = 3 / (2 + 2 + 3) = 3/7\r\n",
    "Now, you can use Naive Bayes to calculate the posterior probabilities for each class:\r\n",
    "\r\n",
    "For class A:\r\n",
    "\r\n",
    "P(A | X1 = 3, X2 = 4) = P(X1 = 3 | A) * P(X2 = 4 | A) * P(A)\r\n",
    "                                 = (4/7) * (3/7) * (1/2)\r\n",
    "                                  = 6/98\r\n",
    "                                  = 3/49\r\n",
    "                      \n",
    "\n",
    "For class B:\r\n",
    "\r\n",
    "P(B | X1 = 3, X2 = 4) = P(X1 = 3 | B) * P(X2 = 4 | B) * P(B)\r\n",
    "                                 = (1/3) * (3/7) * (1/2)\r\n",
    "                                  = 1/14\r\n",
    "                                  = 0.07142\r\n",
    "The Probabilty of class Prediction is calculated as,\r\n",
    "\r\n",
    "P(A | X1 = 3, X2 = 4) = 0.06122 / (0.06122 + 0.07142)\r\n",
    "                                 = 0.4615 * 100\r\n",
    "                                 = 46.15%\r\n",
    "\r\n",
    "P(B | X1 = 3, X2 = 4) = 07142 / (0.06122 + 0.07142)\r\n",
    "                                 = 0.5384 * 100\r\n",
    "                                 = 53.84%\r\n",
    "\r\n",
    "Since Probability of Class B is 53.84 which is greater then Class A of 46.15 , Naive Bayes would predict that the new instance with features X1 = 3 and X2 = 4 belongs to class B.                                          = 0.06122"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
